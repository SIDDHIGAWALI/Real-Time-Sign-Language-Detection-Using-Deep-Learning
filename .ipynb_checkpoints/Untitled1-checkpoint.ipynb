{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19d75d1-db1d-4c31-90a0-dd69aa7b2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet ASL Detection Started. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ASL Classes\n",
    "ASL_CLASSES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "               'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "               'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "def create_resnet_model(num_classes):\n",
    "    \"\"\"Create ResNet18 model\"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def load_resnet_model(model_path, num_classes=29):\n",
    "    \"\"\"Load trained ResNet model\"\"\"\n",
    "    model = create_resnet_model(num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_frame(frame, image_size=224):\n",
    "    \"\"\"Preprocess frame for ResNet\"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # Apply transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    tensor = transform(pil_image).unsqueeze(0)\n",
    "    return tensor\n",
    "\n",
    "def run_resnet_detection():\n",
    "    \"\"\"Run real-time ASL detection with ResNet\"\"\"\n",
    "    # Load model\n",
    "    model_path = \"models/asl_resnet_model.pth\"\n",
    "    model = load_resnet_model(model_path)\n",
    "    \n",
    "    # Initialize camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    print(\"ResNet ASL Detection Started. Press 'q' to quit.\")\n",
    "    \n",
    "    # For prediction smoothing\n",
    "    prediction_history = []\n",
    "    history_size = 5\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Create ROI for hand detection\n",
    "        h, w = frame.shape[:2]\n",
    "        roi_size = 300\n",
    "        x1 = (w - roi_size) // 2\n",
    "        y1 = (h - roi_size) // 2\n",
    "        x2 = x1 + roi_size\n",
    "        y2 = y1 + roi_size\n",
    "        \n",
    "        # Extract ROI\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Preprocess and predict\n",
    "        input_tensor = preprocess_frame(roi)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            confidence, predicted = torch.max(probabilities, 1)\n",
    "            \n",
    "            predicted_class = ASL_CLASSES[predicted.item()]\n",
    "            confidence_score = confidence.item()\n",
    "        \n",
    "        # Smooth predictions\n",
    "        prediction_history.append((predicted_class, confidence_score))\n",
    "        if len(prediction_history) > history_size:\n",
    "            prediction_history.pop(0)\n",
    "        \n",
    "        # Get most frequent prediction if confidence > 0.7\n",
    "        if confidence_score > 0.7:\n",
    "            most_common = max(set([p[0] for p in prediction_history]), \n",
    "                            key=[p[0] for p in prediction_history].count)\n",
    "            display_text = f\"Prediction: {most_common} ({confidence_score:.2f})\"\n",
    "        else:\n",
    "            display_text = \"Low Confidence\"\n",
    "        \n",
    "        # Draw ROI rectangle\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display prediction\n",
    "        cv2.putText(frame, display_text, (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, \"ResNet Model\", (10, 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.putText(frame, \"Place hand in green box\", (10, h-20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        cv2.imshow('ASL Detection - ResNet', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_resnet_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e005bb3b-79dd-479b-9292-020d27c7c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL Detection with Multiple Models\n",
      "1. ResNet Model\n",
      "2. Vision Transformer (ViT) Model\n",
      "3. Graph Neural Network (GNN) Model\n",
      "4. All Models Together\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1-4):  4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"patch_embed.projection.weight\", \"patch_embed.projection.bias\", \"blocks.0.attn.qkv.weight\", \"blocks.0.attn.qkv.bias\", \"blocks.0.attn.proj.weight\", \"blocks.0.attn.proj.bias\", \"blocks.1.attn.qkv.weight\", \"blocks.1.attn.qkv.bias\", \"blocks.1.attn.proj.weight\", \"blocks.1.attn.proj.bias\", \"blocks.2.attn.qkv.weight\", \"blocks.2.attn.qkv.bias\", \"blocks.2.attn.proj.weight\", \"blocks.2.attn.proj.bias\", \"blocks.3.norm1.weight\", \"blocks.3.norm1.bias\", \"blocks.3.attn.qkv.weight\", \"blocks.3.attn.qkv.bias\", \"blocks.3.attn.proj.weight\", \"blocks.3.attn.proj.bias\", \"blocks.3.norm2.weight\", \"blocks.3.norm2.bias\", \"blocks.3.mlp.0.weight\", \"blocks.3.mlp.0.bias\", \"blocks.3.mlp.3.weight\", \"blocks.3.mlp.3.bias\", \"blocks.4.norm1.weight\", \"blocks.4.norm1.bias\", \"blocks.4.attn.qkv.weight\", \"blocks.4.attn.qkv.bias\", \"blocks.4.attn.proj.weight\", \"blocks.4.attn.proj.bias\", \"blocks.4.norm2.weight\", \"blocks.4.norm2.bias\", \"blocks.4.mlp.0.weight\", \"blocks.4.mlp.0.bias\", \"blocks.4.mlp.3.weight\", \"blocks.4.mlp.3.bias\", \"blocks.5.norm1.weight\", \"blocks.5.norm1.bias\", \"blocks.5.attn.qkv.weight\", \"blocks.5.attn.qkv.bias\", \"blocks.5.attn.proj.weight\", \"blocks.5.attn.proj.bias\", \"blocks.5.norm2.weight\", \"blocks.5.norm2.bias\", \"blocks.5.mlp.0.weight\", \"blocks.5.mlp.0.bias\", \"blocks.5.mlp.3.weight\", \"blocks.5.mlp.3.bias\". \n\tUnexpected key(s) in state_dict: \"patch_embed.proj.weight\", \"patch_embed.proj.bias\", \"blocks.0.attn.in_proj_weight\", \"blocks.0.attn.in_proj_bias\", \"blocks.0.attn.out_proj.weight\", \"blocks.0.attn.out_proj.bias\", \"blocks.1.attn.in_proj_weight\", \"blocks.1.attn.in_proj_bias\", \"blocks.1.attn.out_proj.weight\", \"blocks.1.attn.out_proj.bias\", \"blocks.2.attn.in_proj_weight\", \"blocks.2.attn.in_proj_bias\", \"blocks.2.attn.out_proj.weight\", \"blocks.2.attn.out_proj.bias\". \n\tsize mismatch for cls_token: copying a param with shape torch.Size([1, 1, 128]) from checkpoint, the shape in current model is torch.Size([1, 1, 384]).\n\tsize mismatch for pos_embed: copying a param with shape torch.Size([1, 65, 128]) from checkpoint, the shape in current model is torch.Size([1, 197, 384]).\n\tsize mismatch for blocks.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.mlp.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for blocks.0.mlp.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for blocks.0.mlp.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for blocks.0.mlp.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.mlp.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for blocks.1.mlp.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for blocks.1.mlp.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for blocks.1.mlp.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.mlp.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for blocks.2.mlp.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for blocks.2.mlp.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for blocks.2.mlp.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([29, 128]) from checkpoint, the shape in current model is torch.Size([29, 384]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 631\u001b[0m\n\u001b[0;32m    629\u001b[0m     run_gnn_detection()\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 631\u001b[0m     \u001b[43mrun_multi_model_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid choice. Running ResNet model by default.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 496\u001b[0m, in \u001b[0;36mrun_multi_model_detection\u001b[1;34m()\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# Load all models\u001b[39;00m\n\u001b[0;32m    495\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m load_resnet_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/asl_resnet_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 496\u001b[0m vit_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_vit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/asl_vit_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m gnn_model \u001b[38;5;241m=\u001b[39m load_gnn_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/asl_gnn_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    499\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 163\u001b[0m, in \u001b[0;36mload_vit_model\u001b[1;34m(model_path, num_classes)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load trained ViT model\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m model \u001b[38;5;241m=\u001b[39m create_vit_model(num_classes)\n\u001b[1;32m--> 163\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2620\u001b[0m             ),\n\u001b[0;32m   2621\u001b[0m         )\n\u001b[0;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2627\u001b[0m         )\n\u001b[0;32m   2628\u001b[0m     )\n\u001b[0;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"patch_embed.projection.weight\", \"patch_embed.projection.bias\", \"blocks.0.attn.qkv.weight\", \"blocks.0.attn.qkv.bias\", \"blocks.0.attn.proj.weight\", \"blocks.0.attn.proj.bias\", \"blocks.1.attn.qkv.weight\", \"blocks.1.attn.qkv.bias\", \"blocks.1.attn.proj.weight\", \"blocks.1.attn.proj.bias\", \"blocks.2.attn.qkv.weight\", \"blocks.2.attn.qkv.bias\", \"blocks.2.attn.proj.weight\", \"blocks.2.attn.proj.bias\", \"blocks.3.norm1.weight\", \"blocks.3.norm1.bias\", \"blocks.3.attn.qkv.weight\", \"blocks.3.attn.qkv.bias\", \"blocks.3.attn.proj.weight\", \"blocks.3.attn.proj.bias\", \"blocks.3.norm2.weight\", \"blocks.3.norm2.bias\", \"blocks.3.mlp.0.weight\", \"blocks.3.mlp.0.bias\", \"blocks.3.mlp.3.weight\", \"blocks.3.mlp.3.bias\", \"blocks.4.norm1.weight\", \"blocks.4.norm1.bias\", \"blocks.4.attn.qkv.weight\", \"blocks.4.attn.qkv.bias\", \"blocks.4.attn.proj.weight\", \"blocks.4.attn.proj.bias\", \"blocks.4.norm2.weight\", \"blocks.4.norm2.bias\", \"blocks.4.mlp.0.weight\", \"blocks.4.mlp.0.bias\", \"blocks.4.mlp.3.weight\", \"blocks.4.mlp.3.bias\", \"blocks.5.norm1.weight\", \"blocks.5.norm1.bias\", \"blocks.5.attn.qkv.weight\", \"blocks.5.attn.qkv.bias\", \"blocks.5.attn.proj.weight\", \"blocks.5.attn.proj.bias\", \"blocks.5.norm2.weight\", \"blocks.5.norm2.bias\", \"blocks.5.mlp.0.weight\", \"blocks.5.mlp.0.bias\", \"blocks.5.mlp.3.weight\", \"blocks.5.mlp.3.bias\". \n\tUnexpected key(s) in state_dict: \"patch_embed.proj.weight\", \"patch_embed.proj.bias\", \"blocks.0.attn.in_proj_weight\", \"blocks.0.attn.in_proj_bias\", \"blocks.0.attn.out_proj.weight\", \"blocks.0.attn.out_proj.bias\", \"blocks.1.attn.in_proj_weight\", \"blocks.1.attn.in_proj_bias\", \"blocks.1.attn.out_proj.weight\", \"blocks.1.attn.out_proj.bias\", \"blocks.2.attn.in_proj_weight\", \"blocks.2.attn.in_proj_bias\", \"blocks.2.attn.out_proj.weight\", \"blocks.2.attn.out_proj.bias\". \n\tsize mismatch for cls_token: copying a param with shape torch.Size([1, 1, 128]) from checkpoint, the shape in current model is torch.Size([1, 1, 384]).\n\tsize mismatch for pos_embed: copying a param with shape torch.Size([1, 65, 128]) from checkpoint, the shape in current model is torch.Size([1, 197, 384]).\n\tsize mismatch for blocks.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.0.mlp.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for blocks.0.mlp.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for blocks.0.mlp.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for blocks.0.mlp.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.1.mlp.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for blocks.1.mlp.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for blocks.1.mlp.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for blocks.1.mlp.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for blocks.2.mlp.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for blocks.2.mlp.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for blocks.2.mlp.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for blocks.2.mlp.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([29, 128]) from checkpoint, the shape in current model is torch.Size([29, 384])."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from collections import deque, Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ASL Classes\n",
    "ASL_CLASSES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "               'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "               'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "class FingerSpellingEngine:\n",
    "    def __init__(self):\n",
    "        self.current_word = \"\"\n",
    "        self.sentence = \"\"\n",
    "        self.word_history = []\n",
    "        self.last_prediction = \"\"\n",
    "        self.last_prediction_time = 0\n",
    "        self.prediction_hold_time = 1.5  # seconds to hold a prediction\n",
    "        self.delete_hold_time = 2.0  # seconds to hold delete\n",
    "        self.space_hold_time = 1.0   # seconds to hold space\n",
    "        \n",
    "        # Load common words dictionary for auto-correct suggestions\n",
    "        self.common_words = self.load_common_words()\n",
    "        \n",
    "        # Prediction smoothing\n",
    "        self.prediction_buffer = deque(maxlen=10)\n",
    "        self.confidence_threshold = 0.75\n",
    "        \n",
    "    def load_common_words(self):\n",
    "        \"\"\"Load common English words for auto-correction\"\"\"\n",
    "        # You can replace this with a file containing common words\n",
    "        common_words = [\n",
    "            'hello', 'world', 'how', 'are', 'you', 'what', 'when', 'where', \n",
    "            'why', 'who', 'good', 'bad', 'yes', 'no', 'please', 'thank', \n",
    "            'sorry', 'help', 'love', 'like', 'want', 'need', 'time', 'day',\n",
    "            'night', 'morning', 'afternoon', 'evening', 'food', 'water',\n",
    "            'home', 'work', 'school', 'friend', 'family', 'happy', 'sad',\n",
    "            'the', 'and', 'for', 'with', 'this', 'that', 'have', 'will',\n",
    "            'can', 'could', 'should', 'would', 'make', 'take', 'give',\n",
    "            'get', 'go', 'come', 'see', 'know', 'think', 'feel', 'look'\n",
    "        ]\n",
    "        return set(common_words)\n",
    "    \n",
    "    def smooth_prediction(self, predicted_class, confidence):\n",
    "        \"\"\"Smooth predictions using a buffer\"\"\"\n",
    "        self.prediction_buffer.append((predicted_class, confidence))\n",
    "        \n",
    "        # Only consider high confidence predictions\n",
    "        high_conf_predictions = [pred for pred, conf in self.prediction_buffer if conf > self.confidence_threshold]\n",
    "        \n",
    "        if len(high_conf_predictions) < 3:  # Need at least 3 consistent predictions\n",
    "            return None, 0\n",
    "        \n",
    "        # Get most common prediction\n",
    "        counter = Counter(high_conf_predictions)\n",
    "        most_common_pred, count = counter.most_common(1)[0]\n",
    "        \n",
    "        # Calculate average confidence for the most common prediction\n",
    "        avg_confidence = np.mean([conf for pred, conf in self.prediction_buffer if pred == most_common_pred])\n",
    "        \n",
    "        return most_common_pred, avg_confidence\n",
    "    \n",
    "    def process_prediction(self, predicted_class, confidence):\n",
    "        \"\"\"Process the prediction and update word/sentence\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Smooth the prediction\n",
    "        smoothed_pred, smoothed_conf = self.smooth_prediction(predicted_class, confidence)\n",
    "        \n",
    "        if smoothed_pred is None:\n",
    "            return\n",
    "        \n",
    "        # Check if prediction has been held long enough\n",
    "        if smoothed_pred == self.last_prediction:\n",
    "            hold_time = current_time - self.last_prediction_time\n",
    "            \n",
    "            if smoothed_pred == 'del' and hold_time > self.delete_hold_time:\n",
    "                self.handle_delete()\n",
    "                self.last_prediction_time = current_time  # Reset timer\n",
    "                \n",
    "            elif smoothed_pred == 'space' and hold_time > self.space_hold_time:\n",
    "                self.handle_space()\n",
    "                self.last_prediction_time = current_time  # Reset timer\n",
    "                \n",
    "            elif smoothed_pred not in ['del', 'space', 'nothing'] and hold_time > self.prediction_hold_time:\n",
    "                self.add_letter(smoothed_pred)\n",
    "                self.last_prediction_time = current_time  # Reset timer\n",
    "        else:\n",
    "            # New prediction\n",
    "            self.last_prediction = smoothed_pred\n",
    "            self.last_prediction_time = current_time\n",
    "    \n",
    "    def add_letter(self, letter):\n",
    "        \"\"\"Add letter to current word\"\"\"\n",
    "        if letter not in ['del', 'space', 'nothing']:\n",
    "            self.current_word += letter.lower()\n",
    "    \n",
    "    def handle_delete(self):\n",
    "        \"\"\"Handle delete action\"\"\"\n",
    "        if self.current_word:\n",
    "            self.current_word = self.current_word[:-1]\n",
    "        elif self.sentence:\n",
    "            # If no current word, remove last character from sentence\n",
    "            self.sentence = self.sentence[:-1]\n",
    "    \n",
    "    def handle_space(self):\n",
    "        \"\"\"Handle space action - finalize word and add to sentence\"\"\"\n",
    "        if self.current_word:\n",
    "            # Add word to history\n",
    "            self.word_history.append(self.current_word)\n",
    "            \n",
    "            # Add to sentence\n",
    "            if self.sentence:\n",
    "                self.sentence += \" \" + self.current_word\n",
    "            else:\n",
    "                self.sentence = self.current_word\n",
    "            \n",
    "            # Clear current word\n",
    "            self.current_word = \"\"\n",
    "    \n",
    "    def get_word_suggestions(self):\n",
    "        \"\"\"Get word suggestions based on current partial word\"\"\"\n",
    "        if len(self.current_word) < 2:\n",
    "            return []\n",
    "        \n",
    "        suggestions = []\n",
    "        for word in self.common_words:\n",
    "            if word.startswith(self.current_word.lower()):\n",
    "                suggestions.append(word)\n",
    "        \n",
    "        return sorted(suggestions)[:3]  # Return top 3 suggestions\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get current status for display\"\"\"\n",
    "        return {\n",
    "            'current_word': self.current_word,\n",
    "            'sentence': self.sentence,\n",
    "            'suggestions': self.get_word_suggestions(),\n",
    "            'last_prediction': self.last_prediction,\n",
    "            'word_count': len(self.word_history)\n",
    "        }\n",
    "    \n",
    "    def clear_all(self):\n",
    "        \"\"\"Clear everything\"\"\"\n",
    "        self.current_word = \"\"\n",
    "        self.sentence = \"\"\n",
    "        self.word_history = []\n",
    "    \n",
    "    def undo_last_word(self):\n",
    "        \"\"\"Undo last word\"\"\"\n",
    "        if self.word_history:\n",
    "            removed_word = self.word_history.pop()\n",
    "            # Rebuild sentence without last word\n",
    "            self.sentence = \" \".join(self.word_history)\n",
    "    \n",
    "    def save_session(self, filename=\"asl_session.json\"):\n",
    "        \"\"\"Save current session\"\"\"\n",
    "        session_data = {\n",
    "            'sentence': self.sentence,\n",
    "            'word_history': self.word_history,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(session_data, f)\n",
    "    \n",
    "    def load_session(self, filename=\"asl_session.json\"):\n",
    "        \"\"\"Load previous session\"\"\"\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                session_data = json.load(f)\n",
    "                self.sentence = session_data.get('sentence', '')\n",
    "                self.word_history = session_data.get('word_history', [])\n",
    "\n",
    "def create_resnet_model(num_classes):\n",
    "    \"\"\"Create ResNet18 model\"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def load_resnet_model(model_path, num_classes=29):\n",
    "    \"\"\"Load trained ResNet model\"\"\"\n",
    "    model = create_resnet_model(num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_frame(frame, image_size=224):\n",
    "    \"\"\"Preprocess frame for ResNet\"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # Apply transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    tensor = transform(pil_image).unsqueeze(0)\n",
    "    return tensor\n",
    "\n",
    "def draw_ui(frame, spelling_engine, predicted_class, confidence, fps):\n",
    "    \"\"\"Draw enhanced UI with finger spelling information\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    status = spelling_engine.get_status()\n",
    "    \n",
    "    # Draw semi-transparent background for text\n",
    "    overlay = frame.copy()\n",
    "    \n",
    "    # Top panel for current prediction\n",
    "    cv2.rectangle(overlay, (0, 0), (w, 120), (0, 0, 0), -1)\n",
    "    \n",
    "    # Bottom panel for sentence building\n",
    "    cv2.rectangle(overlay, (0, h-200), (w, h), (0, 0, 0), -1)\n",
    "    \n",
    "    # Blend overlay\n",
    "    alpha = 0.7\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "    \n",
    "    # Current prediction with confidence\n",
    "    pred_text = f\"Sign: {predicted_class} ({confidence:.2f})\"\n",
    "    cv2.putText(frame, pred_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    \n",
    "    # FPS\n",
    "    cv2.putText(frame, f\"FPS: {fps:.1f}\", (w-100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    # Current word being spelled\n",
    "    word_text = f\"Current Word: {status['current_word']}_\"\n",
    "    cv2.putText(frame, word_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "    \n",
    "    # Word suggestions\n",
    "    suggestions = status['suggestions']\n",
    "    if suggestions:\n",
    "        sugg_text = f\"Suggestions: {', '.join(suggestions)}\"\n",
    "        cv2.putText(frame, sugg_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "    \n",
    "    # Sentence display (bottom panel)\n",
    "    sentence_y_start = h - 180\n",
    "    \n",
    "    # Title\n",
    "    cv2.putText(frame, \"Sentence:\", (10, sentence_y_start), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    # Sentence text (wrap if too long)\n",
    "    sentence = status['sentence']\n",
    "    if len(sentence) > 80:  # Wrap long sentences\n",
    "        lines = [sentence[i:i+80] for i in range(0, len(sentence), 80)]\n",
    "        for i, line in enumerate(lines[-3:]):  # Show last 3 lines\n",
    "            cv2.putText(frame, line, (10, sentence_y_start + 30 + i*25), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "    else:\n",
    "        cv2.putText(frame, sentence, (10, sentence_y_start + 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "    \n",
    "    # Instructions\n",
    "    instructions = [\n",
    "        \"Hold gesture for 1.5s to add letter\",\n",
    "        \"Hold SPACE for 1s to add word\",\n",
    "        \"Hold DEL for 2s to delete\",\n",
    "        \"Keys: C-clear, U-undo, S-save, Q-quit\"\n",
    "    ]\n",
    "    \n",
    "    start_y = sentence_y_start + 80\n",
    "    for i, instruction in enumerate(instructions):\n",
    "        cv2.putText(frame, instruction, (10, start_y + i*20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)\n",
    "    \n",
    "    # Progress indicator for held gestures\n",
    "    if status['last_prediction'] in ['del', 'space'] or (status['last_prediction'] not in ['nothing']):\n",
    "        current_time = time.time()\n",
    "        hold_time = current_time - spelling_engine.last_prediction_time\n",
    "        \n",
    "        if status['last_prediction'] == 'del':\n",
    "            required_time = spelling_engine.delete_hold_time\n",
    "            color = (0, 0, 255)  # Red\n",
    "        elif status['last_prediction'] == 'space':\n",
    "            required_time = spelling_engine.space_hold_time\n",
    "            color = (255, 0, 0)  # Blue\n",
    "        else:\n",
    "            required_time = spelling_engine.prediction_hold_time\n",
    "            color = (0, 255, 0)  # Green\n",
    "        \n",
    "        if hold_time < required_time:\n",
    "            progress = hold_time / required_time\n",
    "            bar_width = int(200 * progress)\n",
    "            cv2.rectangle(frame, (w-220, 50), (w-220 + bar_width, 70), color, -1)\n",
    "            cv2.rectangle(frame, (w-220, 50), (w-20, 70), (255, 255, 255), 2)\n",
    "\n",
    "def run_enhanced_asl_detection():\n",
    "    \"\"\"Run enhanced ASL detection with finger spelling\"\"\"\n",
    "    # Load model\n",
    "    model_path = \"models/asl_resnet_model.pth\"\n",
    "    \n",
    "    try:\n",
    "        model = load_resnet_model(model_path)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Make sure the model file exists at: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize finger spelling engine\n",
    "    spelling_engine = FingerSpellingEngine()\n",
    "    \n",
    "    # Try to load previous session\n",
    "    spelling_engine.load_session()\n",
    "    \n",
    "    # Initialize camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera\")\n",
    "        return\n",
    "    \n",
    "    print(\"Enhanced ASL Detection with Finger Spelling Started!\")\n",
    "    print(\"Controls:\")\n",
    "    print(\"  Hold gestures to spell words\")\n",
    "    print(\"  C - Clear all\")\n",
    "    print(\"  U - Undo last word\")\n",
    "    print(\"  S - Save session\")\n",
    "    print(\"  Q - Quit\")\n",
    "    \n",
    "    # FPS calculation\n",
    "    fps_counter = 0\n",
    "    fps_start_time = time.time()\n",
    "    current_fps = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame\")\n",
    "            break\n",
    "        \n",
    "        # Flip frame horizontally for mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Calculate FPS\n",
    "        fps_counter += 1\n",
    "        if fps_counter % 30 == 0:  # Update FPS every 30 frames\n",
    "            current_fps = 30 / (time.time() - fps_start_time)\n",
    "            fps_start_time = time.time()\n",
    "        \n",
    "        # Create ROI for hand detection\n",
    "        h, w = frame.shape[:2]\n",
    "        roi_size = 300\n",
    "        x1 = (w - roi_size) // 2\n",
    "        y1 = (h - roi_size) // 2 - 50  # Move ROI up a bit\n",
    "        x2 = x1 + roi_size\n",
    "        y2 = y1 + roi_size\n",
    "        \n",
    "        # Extract ROI\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Preprocess and predict\n",
    "        try:\n",
    "            input_tensor = preprocess_frame(roi)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence, predicted = torch.max(probabilities, 1)\n",
    "                \n",
    "                predicted_class = ASL_CLASSES[predicted.item()]\n",
    "                confidence_score = confidence.item()\n",
    "            \n",
    "            # Process prediction with spelling engine\n",
    "            spelling_engine.process_prediction(predicted_class, confidence_score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            predicted_class = \"error\"\n",
    "            confidence_score = 0\n",
    "        \n",
    "        # Draw ROI rectangle\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw enhanced UI\n",
    "        draw_ui(frame, spelling_engine, predicted_class, confidence_score, current_fps)\n",
    "        \n",
    "        cv2.imshow('Enhanced ASL Detection - Finger Spelling', frame)\n",
    "        \n",
    "        # Handle keyboard input\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('c'):\n",
    "            spelling_engine.clear_all()\n",
    "            print(\"Cleared all text\")\n",
    "        elif key == ord('u'):\n",
    "            spelling_engine.undo_last_word()\n",
    "            print(\"Undid last word\")\n",
    "        elif key == ord('s'):\n",
    "            spelling_engine.save_session()\n",
    "            print(\"Session saved\")\n",
    "        elif key == ord('l'):\n",
    "            spelling_engine.load_session()\n",
    "            print(\"Session loaded\")\n",
    "    \n",
    "    # Save session before quitting\n",
    "    spelling_engine.save_session()\n",
    "    print(f\"Final sentence: {spelling_engine.sentence}\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_enhanced_asl_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
